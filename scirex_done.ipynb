{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uyEMbQrVUrL",
        "outputId": "e1f71631-c1fa-4bb1-fca1-3bad3a21f2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.8/dist-packages (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install Click"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwhq3IIyV3yO",
        "outputId": "5bd27337-f258-484f-d566-43a32713fcc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.6.1-py3-none-any.whl (385 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.4/385.4 KB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elastic-transport<9,>=8\n",
            "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<2,>=1.26.2\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elastic-transport<9,>=8->elasticsearch) (2022.12.7)\n",
            "Installing collected packages: urllib3, elastic-transport, elasticsearch\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed elastic-transport-8.4.0 elasticsearch-8.6.1 urllib3-1.26.14\n"
          ]
        }
      ],
      "source": [
        "!pip install elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TktjmfxvV94y",
        "outputId": "f7505349-f183-4d06-d2e9-1b44ef728317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhvDF6FdWD_J",
        "outputId": "6e9fe202-b33c-438a-81df-525153544b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scripts\n",
            "  Downloading scripts-2.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting stua\n",
            "  Downloading stua-0.2-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: stua, scripts\n",
            "Successfully installed scripts-2.0 stua-0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7O5Ijm9WSvK",
        "outputId": "d7c5b4a6-6870-43ff-cf52-006acc62dc0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.11-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.8/dist-packages (from ipdb) (2.0.1)\n",
            "Collecting ipython>=7.31.1\n",
            "  Downloading ipython-8.10.0-py3-none-any.whl (784 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m784.3/784.3 KB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipdb) (4.4.2)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.30\n",
            "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 KB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb) (4.8.0)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb) (2.6.1)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stack-data\n",
            "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=7.31.1->ipdb) (0.2.6)\n",
            "Collecting executing>=1.2.0\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting pure-eval\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting asttokens>=2.1.0\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=7.31.1->ipdb) (1.15.0)\n",
            "Installing collected packages: pure-eval, executing, prompt-toolkit, matplotlib-inline, jedi, asttokens, stack-data, ipython, ipdb\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 2.0.10\n",
            "    Uninstalling prompt-toolkit-2.0.10:\n",
            "      Successfully uninstalled prompt-toolkit-2.0.10\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.9.0\n",
            "    Uninstalling ipython-7.9.0:\n",
            "      Successfully uninstalled ipython-7.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asttokens-2.2.1 executing-1.2.0 ipdb-0.13.11 ipython-8.10.0 jedi-0.18.2 matplotlib-inline-0.1.6 prompt-toolkit-3.0.36 pure-eval-0.2.2 stack-data-0.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyw63CbZW3fL",
        "outputId": "a98e8b7e-aa13-4d95-e609-ebc8a6d2dd01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SciREX'...\n",
            "remote: Enumerating objects: 2446, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 2446 (delta 130), reused 126 (delta 119), pack-reused 2284\u001b[K\n",
            "Receiving objects: 100% (2446/2446), 45.90 MiB | 16.10 MiB/s, done.\n",
            "Resolving deltas: 100% (904/904), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cAmartya/SciREX.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpam9RooX8HN",
        "outputId": "323415b1-2ba8-49d8-9ed7-43d918335d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SciREX/scirex_dataset\n"
          ]
        }
      ],
      "source": [
        "cd /content/SciREX/scirex_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM9RqsQVX_Uf",
        "outputId": "5ce80195-564a-4355-aa37-7adebcfffce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-11 10:04:47--  https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.92.193.208, 52.92.193.184, 52.218.154.72, ...\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.92.193.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 410593280 (392M) [application/x-tar]\n",
            "Saving to: ‘scibert_scivocab_uncased.tar’\n",
            "\n",
            "scibert_scivocab_un 100%[===================>] 391.57M  17.0MB/s    in 25s     \n",
            "\n",
            "2023-02-11 10:05:13 (15.4 MB/s) - ‘scibert_scivocab_uncased.tar’ saved [410593280/410593280]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x498961ik4M",
        "outputId": "e7369002-43b2-4acf-b5a2-86b592a52ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scibert_scivocab_uncased/\n",
            "scibert_scivocab_uncased/weights.tar.gz\n",
            "scibert_scivocab_uncased/vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!tar -xvf /content/SciREX/scirex_dataset/scibert_scivocab_uncased.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fshBsxOiarPM",
        "outputId": "42bac0b5-889c-421e-ac23-5008a5b808a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "release_data/\n",
            "release_data/train.jsonl\n",
            "release_data/test.jsonl\n",
            "release_data/dev.jsonl\n"
          ]
        }
      ],
      "source": [
        "!tar -xvf /content/SciREX/scirex_dataset/release_data.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2u9cxighX8X",
        "outputId": "30727c77-ddc1-44c4-e9cd-ce87bb2ef811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SciREX\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKwLmFpwgGw-",
        "outputId": "4e21b269-70c1-4ed5-fd30-171af65fd1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting allennlp==0.9.0\n",
            "  Downloading allennlp-0.9.0-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting overrides==1.9\n",
            "  Downloading overrides-1.9.tar.gz (3.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.16.4\n",
            "  Downloading numpy-1.16.4.zip (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas==0.24.2\n",
            "  Downloading pandas-0.24.2.tar.gz (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scripts==2.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (2.0)\n",
            "Collecting scikit_learn==0.21.3\n",
            "  Downloading scikit-learn-0.21.3.tar.gz (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing==3.7.4\n",
            "  Downloading typing-3.7.4-py3-none-any.whl (25 kB)\n",
            "Collecting Click==7.0\n",
            "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.32.1\n",
            "  Downloading tqdm-4.32.1-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fuzzywuzzy==0.17.0\n",
            "  Downloading fuzzywuzzy-0.17.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting spacy==2.1.6\n",
            "  Downloading spacy-2.1.6.tar.gz (30.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting p_tqdm==1.2\n",
            "  Downloading p_tqdm-1.2.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting beautifulsoup4==4.8.0\n",
            "  Downloading beautifulsoup4-4.8.0-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elasticsearch==7.0.2\n",
            "  Downloading elasticsearch-7.0.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 KB\u001b[0m \u001b[31m282.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipdb\n",
            "  Downloading ipdb-0.13.11-py3-none-any.whl (12 kB)\n",
            "Collecting python-intervals\n",
            "  Downloading python_intervals-1.10.0.post1-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (2.25.1)\n",
            "Collecting parsimonious>=0.8.0\n",
            "  Downloading parsimonious-0.10.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flaky\n",
            "  Downloading flaky-3.7.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flask-cors>=3.0.7\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.6/593.6 KB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (2022.7.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (1.13.1+cu116)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (3.6.4)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "  Downloading pytorch_transformers-1.1.0-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (3.1.0)\n",
            "Collecting responses>=0.7\n",
            "  Downloading responses-0.22.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (3.7)\n",
            "Collecting gevent>=1.3.6\n",
            "  Downloading gevent-22.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (1.1.4)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading conllu-1.3.1-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.26.69-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.8/dist-packages (from allennlp==0.9.0->-r requirements.txt (line 1)) (0.5.3)\n",
            "Collecting jsonpickle\n",
            "  Downloading jsonpickle-3.0.1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.8/dist-packages (from pandas==0.24.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: stua in /usr/local/lib/python3.8/dist-packages (from scripts==2.0->-r requirements.txt (line 5)) (0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==0.21.3->-r requirements.txt (line 6)) (1.2.0)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Using cached plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6->-r requirements.txt (line 12)) (0.10.1)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "  Using cached blis-0.2.4-cp38-cp38-linux_x86_64.whl\n",
            "Collecting srsly<1.1.0,>=0.0.6\n",
            "  Using cached srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6->-r requirements.txt (line 12)) (1.0.9)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "  Using cached preshed-2.0.1-cp38-cp38-linux_x86_64.whl\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "  Using cached thinc-7.0.8-cp38-cp38-linux_x86_64.whl\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6->-r requirements.txt (line 12)) (2.0.7)\n",
            "Collecting pathos\n",
            "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soupsieve>=1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from elasticsearch==7.0.2->-r requirements.txt (line 15)) (1.26.14)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch-transformers==1.1.0->allennlp==0.9.0->-r requirements.txt (line 1)) (2022.6.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython>=7.31.1\n",
            "  Downloading ipython-8.10.0-py3-none-any.whl (784 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m784.3/784.3 KB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.8/dist-packages (from ipdb->-r requirements.txt (line 16)) (2.0.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipdb->-r requirements.txt (line 16)) (4.4.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.0.2->allennlp==0.9.0->-r requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask>=1.0.2->allennlp==0.9.0->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask>=1.0.2->allennlp==0.9.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.8/dist-packages (from flask-cors>=3.0.7->allennlp==0.9.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.6-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from gevent>=1.3.6->allennlp==0.9.0->-r requirements.txt (line 1)) (2.0.2)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 KB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from gevent>=1.3.6->allennlp==0.9.0->-r requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 16)) (0.2.0)\n",
            "Collecting stack-data\n",
            "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.30\n",
            "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 16)) (2.6.1)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 16)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 16)) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.31.1->ipdb->-r requirements.txt (line 16)) (4.8.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0->-r requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0->-r requirements.txt (line 1)) (0.11.0)\n",
            "Collecting sphinx>=4.2\n",
            "  Downloading sphinx-6.1.3-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18->allennlp==0.9.0->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18->allennlp==0.9.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.18->allennlp==0.9.0->-r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from responses>=0.7->allennlp==0.9.0->-r requirements.txt (line 1)) (0.10.2)\n",
            "Collecting types-toml\n",
            "  Downloading types_toml-0.10.8.3-py3-none-any.whl (4.5 kB)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.7.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.7.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.6.3-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.6.2-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.6.1-cp38-cp38-manylinux1_x86_64.whl (27.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.6.0-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.5.4-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=1.2->allennlp==0.9.0->-r requirements.txt (line 1)) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.2.0->allennlp==0.9.0->-r requirements.txt (line 1)) (4.4.0)\n",
            "Collecting botocore<1.30.0,>=1.29.69\n",
            "  Downloading botocore-1.29.69-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->allennlp==0.9.0->-r requirements.txt (line 1)) (0.2.6)\n",
            "Collecting h5py\n",
            "  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from pathos->p_tqdm==1.2->-r requirements.txt (line 13)) (0.3.6)\n",
            "Collecting ppft>=1.7.6.6\n",
            "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess>=0.70.14\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.2\n",
            "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->allennlp==0.9.0->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->allennlp==0.9.0->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->allennlp==0.9.0->-r requirements.txt (line 1)) (22.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->allennlp==0.9.0->-r requirements.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->allennlp==0.9.0->-r requirements.txt (line 1)) (1.11.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb->-r requirements.txt (line 16)) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.0.2->allennlp==0.9.0->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb->-r requirements.txt (line 16)) (0.7.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (0.7.13)\n",
            "Requirement already satisfied: importlib-metadata>=4.8 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (2.2.0)\n",
            "Collecting sphinx>=4.2\n",
            "  Downloading sphinx-6.1.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinx-6.1.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinx-6.1.0-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinx-6.0.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinx-6.0.0-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments>=2.4.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx>=4.2\n",
            "  Downloading sphinx-5.3.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinx-5.2.3-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinx-5.2.2-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinx-5.2.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinx-5.2.0.post0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinx-5.2.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading Sphinx-5.1.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imagesize in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (23.0)\n",
            "Requirement already satisfied: docutils<0.20,>=0.14 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (0.16)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (1.0.3)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (1.0.4)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (2.11.0)\n",
            "Collecting pure-eval\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting asttokens>=2.1.0\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting executing>=1.2.0\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8->sphinx>=4.2->numpydoc>=0.8.0->allennlp==0.9.0->-r requirements.txt (line 1)) (3.12.1)\n",
            "Building wheels for collected packages: overrides, numpy, pandas, scikit_learn, spacy, p_tqdm, jsonnet, word2number\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-1.9-py3-none-any.whl size=4210 sha256=c306235b9303810cce093e97d5649d10b02ca87c76b758e6ca2789c9e2e1c4bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/e4/bd/f1d698d00b4047d73fdbb4ac30b8e489a51527282d913bba5f\n",
            "  Building wheel for numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpy: filename=numpy-1.16.4-cp38-cp38-linux_x86_64.whl size=9913727 sha256=5770bea0334472ebfcaa17405711d54e3369c4e8daa2b2d49c44fbdb26551ba4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/a2/a0/d238cbfb93ae764b3d385de800957faea46d36751def0d0557\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_pKtepAa2Ks",
        "outputId": "65e3b26e-7484-431a-919a-48f55936e9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy==2.1.6 in /usr/local/lib/python3.8/dist-packages (2.1.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (1.0.9)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (1.16.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (2.25.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy==2.1.6) (2.0.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.6) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.6) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.6) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.6) (4.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from thinc<7.1.0,>=7.0.8->spacy==2.1.6) (4.32.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy==2.1.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl9MU3jpl_Xb",
        "outputId": "cfdffc9e-a05e-4e86-95c9-be64ad6c9aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_sm==2.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: en_core_web_sm\n",
            "  Building wheel for en_core_web_sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en_core_web_sm: filename=en_core_web_sm-2.1.0-py3-none-any.whl size=11074432 sha256=7fda815bd0cf5448695419d2c709bb8f952eceb5587ee42e26afc9037f74d5d9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7job2qwo/wheels/83/63/50/6467284e1c2d50ffcf02e6582680fd16c6375748a94f75c1a0\n",
            "Successfully built en_core_web_sm\n",
            "Installing collected packages: en_core_web_sm\n",
            "  Attempting uninstall: en_core_web_sm\n",
            "    Found existing installation: en-core-web-sm 3.4.1\n",
            "    Uninstalling en-core-web-sm-3.4.1:\n",
            "      Successfully uninstalled en-core-web-sm-3.4.1\n",
            "Successfully installed en_core_web_sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.8/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBxfSWUYncFq"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/SciREX/outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xhiIihu45aFu",
        "outputId": "1ed3c74e-0d3e-498e-e9aa-fc720fe4b1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/SciREX'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie7fcLiH8eLJ",
        "outputId": "1af7dd66-b4eb-403e-df8c-56b0e815ef05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.20.0\n",
            "  Downloading numpy-1.20.0-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.21.3 which is incompatible.\n",
            "xarray 2022.12.0 requires pandas>=1.3, but you have pandas 0.24.2 which is incompatible.\n",
            "xarray-einstats 0.5.1 requires scipy>=1.6, but you have scipy 1.5.4 which is incompatible.\n",
            "prophet 1.1.2 requires pandas>=1.0.4, but you have pandas 0.24.2 which is incompatible.\n",
            "prophet 1.1.2 requires tqdm>=4.36.1, but you have tqdm 4.32.1 which is incompatible.\n",
            "plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 0.24.2 which is incompatible.\n",
            "mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 0.24.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.21.3 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.10.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 0.24.2 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T1yNd6Ubb3A",
        "outputId": "f1bf93a0-2c78-405c-c0bb-3442d5370081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 07:24:47,936 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dtype=np.int):\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, positive=False):\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
            "2023-02-11 07:24:48,297 - INFO - pytorch_transformers.modeling_bert - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2023-02-11 07:24:48,300 - INFO - pytorch_transformers.modeling_xlnet - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2023-02-11 07:24:48,660 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2023-02-11 07:24:48,661 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2023-02-11 07:24:48,661 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2023-02-11 07:24:48,662 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  EPS = np.finfo(np.float).eps\n",
            "2023-02-11 07:24:49,443 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2023-02-11 07:24:49,443 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2023-02-11 07:24:49,443 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2023-02-11 07:24:50,062 - INFO - allennlp.common.checks - Pytorch version: 1.13.1+cu116\n",
            "2023-02-11 07:24:50,076 - INFO - allennlp.common.params - evaluate_on_test = True\n",
            "2023-02-11 07:24:50,076 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2023-02-11 07:24:50,076 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'token_indexers': {'bert': {'do_lowercase': 'true', 'pretrained_model': 'scirex_dataset/scibert_scivocab_uncased/vocab.txt', 'truncate_long_sequences': False, 'type': 'bert-pretrained', 'use_starting_offsets': True}}, 'type': 'scirex_full_reader'} and extras set()\n",
            "2023-02-11 07:24:50,076 - INFO - allennlp.common.params - dataset_reader.type = scirex_full_reader\n",
            "2023-02-11 07:24:50,076 - INFO - allennlp.common.from_params - instantiating class <class 'scirex.data.dataset_readers.scirex_full_reader.ScirexFullReader'> from params {'token_indexers': {'bert': {'do_lowercase': 'true', 'pretrained_model': 'scirex_dataset/scibert_scivocab_uncased/vocab.txt', 'truncate_long_sequences': False, 'type': 'bert-pretrained', 'use_starting_offsets': True}}} and extras set()\n",
            "2023-02-11 07:24:50,077 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'do_lowercase': 'true', 'pretrained_model': 'scirex_dataset/scibert_scivocab_uncased/vocab.txt', 'truncate_long_sequences': False, 'type': 'bert-pretrained', 'use_starting_offsets': True} and extras set()\n",
            "2023-02-11 07:24:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = bert-pretrained\n",
            "2023-02-11 07:24:50,077 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer'> from params {'do_lowercase': 'true', 'pretrained_model': 'scirex_dataset/scibert_scivocab_uncased/vocab.txt', 'truncate_long_sequences': False, 'use_starting_offsets': True} and extras set()\n",
            "2023-02-11 07:24:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.pretrained_model = scirex_dataset/scibert_scivocab_uncased/vocab.txt\n",
            "2023-02-11 07:24:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.use_starting_offsets = True\n",
            "2023-02-11 07:24:50,077 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.do_lowercase = true\n",
            "2023-02-11 07:24:50,078 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.never_lowercase = None\n",
            "2023-02-11 07:24:50,078 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_pieces = 512\n",
            "2023-02-11 07:24:50,078 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.truncate_long_sequences = False\n",
            "2023-02-11 07:24:50,078 - INFO - pytorch_pretrained_bert.tokenization - loading vocabulary file scirex_dataset/scibert_scivocab_uncased/vocab.txt\n",
            "2023-02-11 07:24:50,114 - INFO - allennlp.common.params - dataset_reader.max_paragraph_length = 300\n",
            "2023-02-11 07:24:50,115 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2023-02-11 07:24:50,115 - INFO - allennlp.common.params - dataset_reader.to_scirex_converter = False\n",
            "2023-02-11 07:24:50,115 - INFO - allennlp.common.params - train_data_path = scirex_dataset/release_data/train.jsonl\n",
            "2023-02-11 07:24:50,115 - INFO - allennlp.training.util - Reading training data from scirex_dataset/release_data/train.jsonl\n",
            "9064it [00:27, 331.56it/s]\n",
            "2023-02-11 07:25:17,453 - INFO - allennlp.common.params - validation_data_path = scirex_dataset/release_data/dev.jsonl\n",
            "2023-02-11 07:25:17,453 - INFO - allennlp.training.util - Reading validation data from scirex_dataset/release_data/dev.jsonl\n",
            "1912it [00:06, 310.75it/s]\n",
            "2023-02-11 07:25:23,606 - INFO - allennlp.common.params - test_data_path = scirex_dataset/release_data/test.jsonl\n",
            "2023-02-11 07:25:23,606 - INFO - allennlp.training.util - Reading test data from scirex_dataset/release_data/test.jsonl\n",
            "2063it [00:05, 356.53it/s]\n",
            "2023-02-11 07:25:29,402 - INFO - allennlp.training.trainer_pieces - From dataset instances, validation, test, train will be considered for vocabulary creation.\n",
            "2023-02-11 07:25:29,402 - INFO - allennlp.common.params - vocabulary.type = None\n",
            "2023-02-11 07:25:29,402 - INFO - allennlp.common.params - vocabulary.extend = False\n",
            "2023-02-11 07:25:29,403 - INFO - allennlp.common.params - vocabulary.directory_path = None\n",
            "2023-02-11 07:25:29,403 - INFO - allennlp.common.params - vocabulary.min_count = None\n",
            "2023-02-11 07:25:29,403 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None\n",
            "2023-02-11 07:25:29,403 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
            "2023-02-11 07:25:29,403 - INFO - allennlp.common.params - vocabulary.pretrained_files = {}\n",
            "2023-02-11 07:25:29,403 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None\n",
            "2023-02-11 07:25:29,403 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False\n",
            "2023-02-11 07:25:29,403 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None\n",
            "2023-02-11 07:25:29,403 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "13039it [00:00, 20111.16it/s]\n",
            "2023-02-11 07:25:30,052 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'context_layer': {'bidirectional': True, 'hidden_size': 200, 'input_size': 768, 'type': 'lstm'}, 'display_metrics': ['validation_metric'], 'lexical_dropout': 0.2, 'loss_weights': {'ner': '1'}, 'modules': {'ner': {'exact_match': 'false', 'label_encoding': 'BIOUL', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2}}}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets']}, 'token_embedders': {'bert': {'pretrained_model': 'scirex_dataset/scibert_scivocab_uncased/weights.tar.gz', 'requires_grad': '10,11,pooler', 'type': 'bert-pretrained-modified'}}}, 'type': 'scirex_model'} and extras {'vocab'}\n",
            "2023-02-11 07:25:30,052 - INFO - allennlp.common.params - model.type = scirex_model\n",
            "2023-02-11 07:25:30,052 - INFO - allennlp.common.from_params - instantiating class <class 'scirex.models.scirex_model.ScirexModel'> from params {'context_layer': {'bidirectional': True, 'hidden_size': 200, 'input_size': 768, 'type': 'lstm'}, 'display_metrics': ['validation_metric'], 'lexical_dropout': 0.2, 'loss_weights': {'ner': '1'}, 'modules': {'ner': {'exact_match': 'false', 'label_encoding': 'BIOUL', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2}}}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets']}, 'token_embedders': {'bert': {'pretrained_model': 'scirex_dataset/scibert_scivocab_uncased/weights.tar.gz', 'requires_grad': '10,11,pooler', 'type': 'bert-pretrained-modified'}}}} and extras {'vocab'}\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets']}, 'token_embedders': {'bert': {'pretrained_model': 'scirex_dataset/scibert_scivocab_uncased/weights.tar.gz', 'requires_grad': '10,11,pooler', 'type': 'bert-pretrained-modified'}}} and extras {'vocab'}\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.params - model.text_field_embedder.allow_unmatched_keys = True\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'pretrained_model': 'scirex_dataset/scibert_scivocab_uncased/weights.tar.gz', 'requires_grad': '10,11,pooler', 'type': 'bert-pretrained-modified'} and extras {'vocab'}\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.type = bert-pretrained-modified\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.from_params - instantiating class <class 'scirex.models.bert_token_embedder_modified.PretrainedBertEmbedder'> from params {'pretrained_model': 'scirex_dataset/scibert_scivocab_uncased/weights.tar.gz', 'requires_grad': '10,11,pooler'} and extras {'vocab'}\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.pretrained_model = scirex_dataset/scibert_scivocab_uncased/weights.tar.gz\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.requires_grad = 10,11,pooler\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.top_layer_only = False\n",
            "2023-02-11 07:25:30,053 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.scalar_mix_parameters = None\n",
            "2023-02-11 07:25:30,053 - INFO - pytorch_pretrained_bert.modeling - loading archive file scirex_dataset/scibert_scivocab_uncased/weights.tar.gz\n",
            "2023-02-11 07:25:30,054 - INFO - pytorch_pretrained_bert.modeling - extracting archive file scirex_dataset/scibert_scivocab_uncased/weights.tar.gz to temp dir /tmp/tmphcuhz6rs\n",
            "2023-02-11 07:25:33,219 - INFO - pytorch_pretrained_bert.modeling - Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 31090\n",
            "}\n",
            "\n",
            "2023-02-11 07:25:34,797 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'hidden_size': 200, 'input_size': 768, 'type': 'lstm'} and extras {'vocab'}\n",
            "2023-02-11 07:25:34,797 - INFO - allennlp.common.params - model.context_layer.type = lstm\n",
            "2023-02-11 07:25:34,798 - INFO - allennlp.common.params - model.context_layer.batch_first = True\n",
            "2023-02-11 07:25:34,798 - INFO - allennlp.common.params - model.context_layer.stateful = False\n",
            "2023-02-11 07:25:34,798 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2023-02-11 07:25:34,798 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2023-02-11 07:25:34,798 - INFO - allennlp.common.params - model.context_layer.bidirectional = True\n",
            "2023-02-11 07:25:34,798 - INFO - allennlp.common.params - model.context_layer.hidden_size = 200\n",
            "2023-02-11 07:25:34,798 - INFO - allennlp.common.params - model.context_layer.input_size = 768\n",
            "2023-02-11 07:25:34,798 - INFO - allennlp.common.params - model.context_layer.batch_first = True\n",
            "2023-02-11 07:25:34,811 - INFO - allennlp.common.params - model.modules = {'ner': {'exact_match': 'false', 'label_encoding': 'BIOUL', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2}}}\n",
            "2023-02-11 07:25:34,811 - INFO - allennlp.common.params - model.loss_weights = {'ner': '1'}\n",
            "2023-02-11 07:25:34,811 - INFO - allennlp.common.params - model.lexical_dropout = 0.2\n",
            "2023-02-11 07:25:34,811 - INFO - allennlp.common.params - model.display_metrics = ['validation_metric']\n",
            "2023-02-11 07:25:34,812 - INFO - allennlp.common.from_params - instantiating class <class 'scirex.models.ner.ner_crf_tagger.NERTagger'> from params {'exact_match': 'false', 'label_encoding': 'BIOUL', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2}} and extras {'vocab'}\n",
            "2023-02-11 07:25:34,812 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2} and extras {'vocab'}\n",
            "2023-02-11 07:25:34,812 - INFO - allennlp.common.params - ner.mention_feedforward.input_dim = 400\n",
            "2023-02-11 07:25:34,812 - INFO - allennlp.common.params - ner.mention_feedforward.num_layers = 2\n",
            "2023-02-11 07:25:34,812 - INFO - allennlp.common.params - ner.mention_feedforward.hidden_dims = 150\n",
            "2023-02-11 07:25:34,812 - INFO - allennlp.common.params - ner.mention_feedforward.activations = gelu\n",
            "2023-02-11 07:25:34,812 - INFO - allennlp.common.registrable - instantiating registered subclass gelu of <class 'allennlp.nn.activations.Activation'>\n",
            "2023-02-11 07:25:34,813 - INFO - allennlp.common.params - ner.mention_feedforward.dropout = 0.2\n",
            "2023-02-11 07:25:34,814 - INFO - allennlp.common.params - ner.label_namespace = ner_type_labels\n",
            "2023-02-11 07:25:34,814 - INFO - allennlp.common.params - ner.label_encoding = BIOUL\n",
            "2023-02-11 07:25:34,814 - INFO - allennlp.common.params - ner.exact_match = false\n",
            "{0: 'O', 1: 'B-Method', 2: 'L-Method', 3: 'I-Method', 4: 'U-Method', 5: 'B-Task', 6: 'L-Task', 7: 'I-Task', 8: 'B-Metric', 9: 'L-Metric', 10: 'U-Task', 11: 'U-Metric', 12: 'B-Material', 13: 'L-Material', 14: 'I-Metric', 15: 'I-Material', 16: 'U-Material'}\n",
            "2023-02-11 07:25:34,815 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.bias\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.weight\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.bias\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.weight\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers -    _ner_crf._constraint_mask\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers -    _ner_crf.transitions\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers -    _ner_scorer._module.bias\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers -    _ner_scorer._module.weight\n",
            "2023-02-11 07:25:34,816 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2023-02-11 07:25:34,817 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2023-02-11 07:25:34,817 - INFO - allennlp.nn.initializers -    _context_layer._module.bias_hh_l0\n",
            "2023-02-11 07:25:34,817 - INFO - allennlp.nn.initializers -    _context_layer._module.bias_hh_l0_reverse\n",
            "2023-02-11 07:25:34,817 - INFO - allennlp.nn.initializers -    _context_layer._module.bias_ih_l0\n",
            "2023-02-11 07:25:34,817 - INFO - allennlp.nn.initializers -    _context_layer._module.bias_ih_l0_reverse\n",
            "2023-02-11 07:25:34,817 - INFO - allennlp.nn.initializers -    _context_layer._module.weight_hh_l0\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _context_layer._module.weight_hh_l0_reverse\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _context_layer._module.weight_ih_l0\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _context_layer._module.weight_ih_l0_reverse\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _ner._mention_feedforward._module._linear_layers.0.bias\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _ner._mention_feedforward._module._linear_layers.0.weight\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _ner._mention_feedforward._module._linear_layers.1.bias\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _ner._mention_feedforward._module._linear_layers.1.weight\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _ner._ner_crf._constraint_mask\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _ner._ner_crf.transitions\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _ner._ner_scorer._module.bias\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _ner._ner_scorer._module.weight\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.gamma\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11\n",
            "2023-02-11 07:25:34,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias\n",
            "2023-02-11 07:25:34,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias\n",
            "2023-02-11 07:25:34,820 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias\n",
            "2023-02-11 07:25:34,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight\n",
            "2023-02-11 07:25:34,822 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,859 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,859 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,859 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,859 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias\n",
            "2023-02-11 07:25:34,859 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight\n",
            "2023-02-11 07:25:34,859 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,859 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,859 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,861 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight\n",
            "2023-02-11 07:25:34,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight\n",
            "2023-02-11 07:25:34,863 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,865 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias\n",
            "2023-02-11 07:25:34,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight\n",
            "2023-02-11 07:25:34,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias\n",
            "2023-02-11 07:25:34,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight\n",
            "2023-02-11 07:25:34,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias\n",
            "2023-02-11 07:25:34,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight\n",
            "2023-02-11 07:25:34,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias\n",
            "2023-02-11 07:25:34,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight\n",
            "2023-02-11 07:25:34,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias\n",
            "2023-02-11 07:25:34,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight\n",
            "2023-02-11 07:25:34,962 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 4, 'type': 'ie_batch'} and extras set()\n",
            "2023-02-11 07:25:34,962 - INFO - allennlp.common.params - iterator.type = ie_batch\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.from_params - instantiating class <class 'scirex.data.iterators.batch_iterator.BatchIterator'> from params {'batch_size': 4} and extras set()\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - iterator.batch_size = 4\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - iterator.instances_per_epoch = None\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - iterator.max_instances_in_memory = None\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - iterator.cache_instances = False\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - iterator.track_epoch = False\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 4, 'type': 'ie_batch'} and extras set()\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - validation_iterator.type = ie_batch\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.from_params - instantiating class <class 'scirex.data.iterators.batch_iterator.BatchIterator'> from params {'batch_size': 4} and extras set()\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - validation_iterator.batch_size = 4\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - validation_iterator.instances_per_epoch = None\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - validation_iterator.max_instances_in_memory = None\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - validation_iterator.cache_instances = False\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - validation_iterator.track_epoch = False\n",
            "2023-02-11 07:25:34,963 - INFO - allennlp.common.params - validation_iterator.maximum_samples_per_batch = None\n",
            "2023-02-11 07:25:34,964 - INFO - allennlp.common.params - trainer.no_grad = ()\n",
            "2023-02-11 07:25:34,965 - INFO - allennlp.training.trainer_pieces - Following parameters are Frozen  (without gradient):\n",
            "2023-02-11 07:25:34,965 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight\n",
            "2023-02-11 07:25:34,965 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight\n",
            "2023-02-11 07:25:34,965 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,966 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,967 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight\n",
            "2023-02-11 07:25:34,968 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight\n",
            "2023-02-11 07:25:34,969 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias\n",
            "2023-02-11 07:25:35,062 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,063 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight\n",
            "2023-02-11 07:25:35,064 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - _ner._ner_crf._constraint_mask\n",
            "2023-02-11 07:25:35,065 - INFO - allennlp.training.trainer_pieces - Following parameters are Tunable (with gradient):\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight\n",
            "2023-02-11 07:25:35,066 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.gamma\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3\n",
            "2023-02-11 07:25:35,067 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _context_layer._module.weight_ih_l0\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _context_layer._module.weight_hh_l0\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _context_layer._module.bias_ih_l0\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _context_layer._module.bias_hh_l0\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _context_layer._module.weight_ih_l0_reverse\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _context_layer._module.weight_hh_l0_reverse\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _context_layer._module.bias_ih_l0_reverse\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _context_layer._module.bias_hh_l0_reverse\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _ner._mention_feedforward._module._linear_layers.0.weight\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _ner._mention_feedforward._module._linear_layers.0.bias\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _ner._mention_feedforward._module._linear_layers.1.weight\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _ner._mention_feedforward._module._linear_layers.1.bias\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _ner._ner_scorer._module.weight\n",
            "2023-02-11 07:25:35,068 - INFO - allennlp.training.trainer_pieces - _ner._ner_scorer._module.bias\n",
            "2023-02-11 07:25:35,069 - INFO - allennlp.training.trainer_pieces - _ner._ner_crf.transitions\n",
            "2023-02-11 07:25:35,069 - INFO - allennlp.common.params - trainer.patience = 10\n",
            "2023-02-11 07:25:35,069 - INFO - allennlp.common.params - trainer.validation_metric = +validation_metric\n",
            "2023-02-11 07:25:35,069 - INFO - allennlp.common.params - trainer.shuffle = True\n",
            "2023-02-11 07:25:35,069 - INFO - allennlp.common.params - trainer.num_epochs = 3\n",
            "2023-02-11 07:25:35,069 - INFO - allennlp.common.params - trainer.cuda_device = 0\n",
            "2023-02-11 07:25:35,069 - INFO - allennlp.common.params - trainer.grad_norm = 5\n",
            "2023-02-11 07:25:35,069 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2023-02-11 07:25:35,069 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2023-02-11 07:25:37,420 - INFO - allennlp.common.params - trainer.optimizer.type = adam\n",
            "2023-02-11 07:25:37,420 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2023-02-11 07:25:37,420 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2023-02-11 07:25:37,420 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups.0.1.lr = 2e-05\n",
            "2023-02-11 07:25:37,421 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n",
            "2023-02-11 07:25:37,421 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight', '_text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight', '_text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight', '_text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight'], {'lr': 2e-05}\n",
            "2023-02-11 07:25:37,421 - INFO - allennlp.training.optimizers - Group 1: ['_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9', '_ner._mention_feedforward._module._linear_layers.1.weight', '_ner._ner_scorer._module.weight', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3', '_context_layer._module.weight_hh_l0_reverse', '_context_layer._module.bias_hh_l0_reverse', '_ner._ner_crf.transitions', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8', '_context_layer._module.bias_ih_l0', '_ner._mention_feedforward._module._linear_layers.0.weight', '_context_layer._module.bias_ih_l0_reverse', '_context_layer._module.bias_hh_l0', '_context_layer._module.weight_ih_l0', '_ner._mention_feedforward._module._linear_layers.0.bias', '_ner._mention_feedforward._module._linear_layers.1.bias', '_context_layer._module.weight_ih_l0_reverse', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0', '_text_field_embedder.token_embedder_bert._scalar_mix.gamma', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6', '_context_layer._module.weight_hh_l0', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7', '_ner._ner_scorer._module.bias', '_text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5'], {}\n",
            "2023-02-11 07:25:37,421 - INFO - allennlp.training.optimizers - Number of trainable parameters: 16404005\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - trainer.optimizer.infer_type_and_cast = True\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.001\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.registrable - instantiating registered subclass adam of <class 'allennlp.training.optimizers.Optimizer'>\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = reduce_on_plateau\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.registrable - instantiating registered subclass reduce_on_plateau of <class 'allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler'>\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.factor = 0.5\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.mode = max\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.patience = 5\n",
            "2023-02-11 07:25:37,422 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 1\n",
            "2023-02-11 07:25:37,423 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None\n",
            "2023-02-11 07:25:37,423 - INFO - allennlp.common.params - trainer.model_save_interval = None\n",
            "2023-02-11 07:25:37,423 - INFO - allennlp.common.params - trainer.summary_interval = 100\n",
            "2023-02-11 07:25:37,423 - INFO - allennlp.common.params - trainer.histogram_interval = None\n",
            "2023-02-11 07:25:37,423 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True\n",
            "2023-02-11 07:25:37,423 - INFO - allennlp.common.params - trainer.should_log_learning_rate = True\n",
            "2023-02-11 07:25:37,423 - INFO - allennlp.common.params - trainer.log_batch_size_period = None\n",
            "2023-02-11 07:25:37,426 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2023-02-11 07:25:37,426 - INFO - allennlp.training.trainer - Epoch 0/2\n",
            "2023-02-11 07:25:37,426 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3847.656\n",
            "2023-02-11 07:25:37,522 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1310\n",
            "2023-02-11 07:25:38,822 - INFO - root - count    306.000000\n",
            "mean       7.784314\n",
            "std        2.397926\n",
            "min        2.000000\n",
            "25%        7.000000\n",
            "50%        8.000000\n",
            "75%        9.000000\n",
            "max       22.000000\n",
            "2023-02-11 07:25:38,823 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/2382 [00:00<?, ?it/s]2023-02-11 07:25:40,878 - WARNING - allennlp.training.util - Metrics with names beginning with \"_\" will not be logged to the tqdm progress bar.\n",
            "validation_metric: 0.6645, loss: 31.7920 ||: 100%|##########| 2382/2382 [16:59<00:00,  2.60it/s]\n",
            "2023-02-11 07:42:38,258 - INFO - allennlp.training.trainer - Validating\n",
            "2023-02-11 07:42:38,322 - INFO - root - count    66.000000\n",
            "mean      7.560606\n",
            "std       2.524518\n",
            "min       3.000000\n",
            "25%       6.000000\n",
            "50%       7.000000\n",
            "75%       9.000000\n",
            "max      18.000000\n",
            "validation_metric: 0.7308, loss: 18.0269 ||: 100%|##########| 499/499 [01:44<00:00,  4.76it/s]\n",
            "2023-02-11 07:44:23,062 - INFO - allennlp.training.tensorboard_writer -                              Training |  Validation\n",
            "2023-02-11 07:44:23,063 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Metric       |     0.516  |     0.716\n",
            "2023-02-11 07:44:23,064 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Material     |     0.337  |     0.595\n",
            "2023-02-11 07:44:23,064 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Material |     0.414  |     0.573\n",
            "2023-02-11 07:44:23,065 - INFO - allennlp.training.tensorboard_writer - _ner_recall              |     0.619  |     0.734\n",
            "2023-02-11 07:44:23,066 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Task     |     0.602  |     0.688\n",
            "2023-02-11 07:44:23,066 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Material  |     0.537  |     0.552\n",
            "2023-02-11 07:44:23,067 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB            |  3847.656  |       N/A\n",
            "2023-02-11 07:44:23,067 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Method       |     0.692  |     0.757\n",
            "2023-02-11 07:44:23,068 - INFO - allennlp.training.tensorboard_writer - validation_metric        |     0.665  |     0.731\n",
            "2023-02-11 07:44:23,069 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Method    |     0.741  |     0.763\n",
            "2023-02-11 07:44:23,069 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Task         |     0.540  |     0.714\n",
            "2023-02-11 07:44:23,070 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure          |     0.665  |     0.731\n",
            "2023-02-11 07:44:23,071 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Metric    |     0.722  |     0.780\n",
            "2023-02-11 07:44:23,072 - INFO - allennlp.training.tensorboard_writer - _ner_precision           |     0.718  |     0.728\n",
            "2023-02-11 07:44:23,073 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Method   |     0.716  |     0.760\n",
            "2023-02-11 07:44:23,073 - INFO - allennlp.training.tensorboard_writer - loss                     |    31.792  |    18.027\n",
            "2023-02-11 07:44:23,074 - INFO - allennlp.training.tensorboard_writer - _loss_ner                |    31.792  |    18.027\n",
            "2023-02-11 07:44:23,075 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Metric   |     0.602  |     0.747\n",
            "2023-02-11 07:44:23,075 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Task      |     0.681  |     0.663\n",
            "2023-02-11 07:44:23,077 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB          |  1310.000  |       N/A\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:1006: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2023-02-11 07:44:24,192 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/best.th'.\n",
            "2023-02-11 07:44:25,998 - INFO - allennlp.training.trainer - Epoch duration: 0:18:48.571894\n",
            "2023-02-11 07:44:25,998 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:37:37\n",
            "2023-02-11 07:44:25,998 - INFO - allennlp.training.trainer - Epoch 1/2\n",
            "2023-02-11 07:44:25,998 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4112.364\n",
            "2023-02-11 07:44:26,097 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1696\n",
            "2023-02-11 07:44:27,437 - INFO - root - count    306.000000\n",
            "mean       7.784314\n",
            "std        2.397926\n",
            "min        2.000000\n",
            "25%        7.000000\n",
            "50%        8.000000\n",
            "75%        9.000000\n",
            "max       22.000000\n",
            "2023-02-11 07:44:27,438 - INFO - allennlp.training.trainer - Training\n",
            "validation_metric: 0.7310, loss: 15.2825 ||: 100%|##########| 2382/2382 [17:04<00:00,  2.33it/s]\n",
            "2023-02-11 08:01:31,873 - INFO - allennlp.training.trainer - Validating\n",
            "2023-02-11 08:01:31,940 - INFO - root - count    66.000000\n",
            "mean      7.560606\n",
            "std       2.524518\n",
            "min       3.000000\n",
            "25%       6.000000\n",
            "50%       7.000000\n",
            "75%       9.000000\n",
            "max      18.000000\n",
            "validation_metric: 0.7388, loss: 14.7754 ||: 100%|##########| 499/499 [01:42<00:00,  4.86it/s]\n",
            "2023-02-11 08:03:14,617 - INFO - allennlp.training.tensorboard_writer -                              Training |  Validation\n",
            "2023-02-11 08:03:14,618 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Metric       |     0.665  |     0.738\n",
            "2023-02-11 08:03:14,618 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Material     |     0.512  |     0.626\n",
            "2023-02-11 08:03:14,619 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Material |     0.546  |     0.569\n",
            "2023-02-11 08:03:14,620 - INFO - allennlp.training.tensorboard_writer - _ner_recall              |     0.711  |     0.728\n",
            "2023-02-11 08:03:14,620 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Material  |     0.586  |     0.521\n",
            "2023-02-11 08:03:14,621 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Task     |     0.690  |     0.705\n",
            "2023-02-11 08:03:14,622 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB            |  4112.364  |       N/A\n",
            "2023-02-11 08:03:14,622 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Method       |     0.757  |     0.766\n",
            "2023-02-11 08:03:14,623 - INFO - allennlp.training.tensorboard_writer - validation_metric        |     0.731  |     0.739\n",
            "2023-02-11 08:03:14,624 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Method    |     0.779  |     0.761\n",
            "2023-02-11 08:03:14,625 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Task         |     0.659  |     0.639\n",
            "2023-02-11 08:03:14,625 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure          |     0.731  |     0.739\n",
            "2023-02-11 08:03:14,626 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Metric    |     0.747  |     0.804\n",
            "2023-02-11 08:03:14,626 - INFO - allennlp.training.tensorboard_writer - _ner_precision           |     0.753  |     0.750\n",
            "2023-02-11 08:03:14,627 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Method   |     0.768  |     0.763\n",
            "2023-02-11 08:03:14,627 - INFO - allennlp.training.tensorboard_writer - loss                     |    15.283  |    14.775\n",
            "2023-02-11 08:03:14,628 - INFO - allennlp.training.tensorboard_writer - _loss_ner                |    15.283  |    14.775\n",
            "2023-02-11 08:03:14,629 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Metric   |     0.704  |     0.769\n",
            "2023-02-11 08:03:14,630 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Task      |     0.724  |     0.787\n",
            "2023-02-11 08:03:14,630 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB          |  1696.000  |       N/A\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:1006: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2023-02-11 08:03:15,583 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/best.th'.\n",
            "2023-02-11 08:03:17,854 - INFO - allennlp.training.trainer - Epoch duration: 0:18:51.855670\n",
            "2023-02-11 08:03:17,854 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:18:50\n",
            "2023-02-11 08:03:17,854 - INFO - allennlp.training.trainer - Epoch 2/2\n",
            "2023-02-11 08:03:17,854 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4112.632\n",
            "2023-02-11 08:03:17,952 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1696\n",
            "2023-02-11 08:03:19,284 - INFO - root - count    306.000000\n",
            "mean       7.784314\n",
            "std        2.397926\n",
            "min        2.000000\n",
            "25%        7.000000\n",
            "50%        8.000000\n",
            "75%        9.000000\n",
            "max       22.000000\n",
            "2023-02-11 08:03:19,285 - INFO - allennlp.training.trainer - Training\n",
            "validation_metric: 0.7509, loss: 12.4834 ||: 100%|##########| 2382/2382 [17:04<00:00,  2.58it/s]\n",
            "2023-02-11 08:20:23,909 - INFO - allennlp.training.trainer - Validating\n",
            "2023-02-11 08:20:23,977 - INFO - root - count    66.000000\n",
            "mean      7.560606\n",
            "std       2.524518\n",
            "min       3.000000\n",
            "25%       6.000000\n",
            "50%       7.000000\n",
            "75%       9.000000\n",
            "max      18.000000\n",
            "validation_metric: 0.7454, loss: 14.6448 ||: 100%|##########| 499/499 [01:44<00:00,  4.79it/s]\n",
            "2023-02-11 08:22:08,143 - INFO - allennlp.training.tensorboard_writer -                              Training |  Validation\n",
            "2023-02-11 08:22:08,143 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Metric       |     0.705  |     0.683\n",
            "2023-02-11 08:22:08,144 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Material     |     0.580  |     0.547\n",
            "2023-02-11 08:22:08,144 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Material |     0.592  |     0.568\n",
            "2023-02-11 08:22:08,145 - INFO - allennlp.training.tensorboard_writer - _ner_recall              |     0.736  |     0.727\n",
            "2023-02-11 08:22:08,146 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Material  |     0.604  |     0.589\n",
            "2023-02-11 08:22:08,147 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Task     |     0.712  |     0.704\n",
            "2023-02-11 08:22:08,147 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB            |  4112.632  |       N/A\n",
            "2023-02-11 08:22:08,148 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Method       |     0.775  |     0.761\n",
            "2023-02-11 08:22:08,149 - INFO - allennlp.training.tensorboard_writer - validation_metric        |     0.751  |     0.745\n",
            "2023-02-11 08:22:08,149 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Method    |     0.792  |     0.795\n",
            "2023-02-11 08:22:08,150 - INFO - allennlp.training.tensorboard_writer - _ner_recall-Task         |     0.685  |     0.704\n",
            "2023-02-11 08:22:08,151 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure          |     0.751  |     0.745\n",
            "2023-02-11 08:22:08,152 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Metric    |     0.768  |     0.816\n",
            "2023-02-11 08:22:08,152 - INFO - allennlp.training.tensorboard_writer - _ner_precision           |     0.766  |     0.764\n",
            "2023-02-11 08:22:08,153 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Method   |     0.783  |     0.777\n",
            "2023-02-11 08:22:08,154 - INFO - allennlp.training.tensorboard_writer - loss                     |    12.483  |    14.645\n",
            "2023-02-11 08:22:08,154 - INFO - allennlp.training.tensorboard_writer - _loss_ner                |    12.483  |    14.645\n",
            "2023-02-11 08:22:08,155 - INFO - allennlp.training.tensorboard_writer - _ner_f1-measure-Metric   |     0.735  |     0.743\n",
            "2023-02-11 08:22:08,155 - INFO - allennlp.training.tensorboard_writer - _ner_precision-Task      |     0.740  |     0.705\n",
            "2023-02-11 08:22:08,156 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB          |  1696.000  |       N/A\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:1006: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2023-02-11 08:22:09,069 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/best.th'.\n",
            "2023-02-11 08:22:11,356 - INFO - allennlp.training.trainer - Epoch duration: 0:18:53.501949\n",
            "2023-02-11 08:22:11,358 - INFO - allennlp.training.checkpointer - loading best weights\n",
            "2023-02-11 08:22:11,621 - INFO - allennlp.commands.train - The model will be evaluated using the best epoch weights.\n",
            "2023-02-11 08:22:11,623 - INFO - allennlp.training.util - Iterating over dataset\n",
            "2023-02-11 08:22:11,695 - INFO - root - count    66.000000\n",
            "mean      8.166667\n",
            "std       2.383167\n",
            "min       4.000000\n",
            "25%       6.250000\n",
            "50%       8.000000\n",
            "75%       9.000000\n",
            "max      15.000000\n",
            "validation_metric: 0.74, loss: 13.95 ||: 100%|##########| 539/539 [01:57<00:00,  4.56it/s]\n",
            "2023-02-11 08:24:08,821 - INFO - allennlp.models.archival - archiving weights and vocabulary to outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/model.tar.gz\n",
            "2023-02-11 08:24:29,985 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 2,\n",
            "  \"peak_cpu_memory_MB\": 4112.632,\n",
            "  \"peak_gpu_0_memory_MB\": 1696,\n",
            "  \"training_duration\": \"0:56:30.730142\",\n",
            "  \"training_start_epoch\": 0,\n",
            "  \"training_epochs\": 2,\n",
            "  \"epoch\": 2,\n",
            "  \"training__ner_precision-Method\": 0.7919685898796349,\n",
            "  \"training__ner_recall-Method\": 0.7750726959824342,\n",
            "  \"training__ner_f1-measure-Method\": 0.7834295568718103,\n",
            "  \"training__ner_precision-Task\": 0.7400851228477462,\n",
            "  \"training__ner_recall-Task\": 0.685205086870858,\n",
            "  \"training__ner_f1-measure-Task\": 0.7115885416666168,\n",
            "  \"training__ner_precision-Metric\": 0.7675332527206772,\n",
            "  \"training__ner_recall-Metric\": 0.7051471949638956,\n",
            "  \"training__ner_f1-measure-Metric\": 0.7350188169448503,\n",
            "  \"training__ner_precision-Material\": 0.6040793517742387,\n",
            "  \"training__ner_recall-Material\": 0.5797023729722482,\n",
            "  \"training__ner_f1-measure-Material\": 0.5916398713825867,\n",
            "  \"training__ner_precision\": 0.7663221626624503,\n",
            "  \"training__ner_recall\": 0.7360019259794254,\n",
            "  \"training__ner_f1-measure\": 0.750856079463017,\n",
            "  \"training__loss_ner\": 12.483428001403809,\n",
            "  \"training_validation_metric\": 0.750856079463017,\n",
            "  \"training_loss\": 12.483438097853854,\n",
            "  \"training_cpu_memory_MB\": 4112.632,\n",
            "  \"training_gpu_0_memory_MB\": 1696,\n",
            "  \"validation__ner_precision-Method\": 0.7950429656984589,\n",
            "  \"validation__ner_recall-Method\": 0.7606849221988177,\n",
            "  \"validation__ner_f1-measure-Method\": 0.7774845475379734,\n",
            "  \"validation__ner_precision-Task\": 0.7052238805970149,\n",
            "  \"validation__ner_recall-Task\": 0.7036194415718717,\n",
            "  \"validation__ner_f1-measure-Task\": 0.7044207474893381,\n",
            "  \"validation__ner_precision-Metric\": 0.8158168574401665,\n",
            "  \"validation__ner_recall-Metric\": 0.6829268292682927,\n",
            "  \"validation__ner_f1-measure-Metric\": 0.7434803224276414,\n",
            "  \"validation__ner_precision-Material\": 0.5894886363636364,\n",
            "  \"validation__ner_recall-Material\": 0.5471324983520105,\n",
            "  \"validation__ner_f1-measure-Material\": 0.5675213675213175,\n",
            "  \"validation__ner_precision\": 0.764335507083427,\n",
            "  \"validation__ner_recall\": 0.7273699978600471,\n",
            "  \"validation__ner_f1-measure\": 0.7453947368420553,\n",
            "  \"validation__loss_ner\": 14.644784927368164,\n",
            "  \"validation_validation_metric\": 0.7453947368420553,\n",
            "  \"validation_loss\": 14.644786198774655,\n",
            "  \"best_validation__ner_precision-Method\": 0.7950429656984589,\n",
            "  \"best_validation__ner_recall-Method\": 0.7606849221988177,\n",
            "  \"best_validation__ner_f1-measure-Method\": 0.7774845475379734,\n",
            "  \"best_validation__ner_precision-Task\": 0.7052238805970149,\n",
            "  \"best_validation__ner_recall-Task\": 0.7036194415718717,\n",
            "  \"best_validation__ner_f1-measure-Task\": 0.7044207474893381,\n",
            "  \"best_validation__ner_precision-Metric\": 0.8158168574401665,\n",
            "  \"best_validation__ner_recall-Metric\": 0.6829268292682927,\n",
            "  \"best_validation__ner_f1-measure-Metric\": 0.7434803224276414,\n",
            "  \"best_validation__ner_precision-Material\": 0.5894886363636364,\n",
            "  \"best_validation__ner_recall-Material\": 0.5471324983520105,\n",
            "  \"best_validation__ner_f1-measure-Material\": 0.5675213675213175,\n",
            "  \"best_validation__ner_precision\": 0.764335507083427,\n",
            "  \"best_validation__ner_recall\": 0.7273699978600471,\n",
            "  \"best_validation__ner_f1-measure\": 0.7453947368420553,\n",
            "  \"best_validation__loss_ner\": 14.644784927368164,\n",
            "  \"best_validation_validation_metric\": 0.7453947368420553,\n",
            "  \"best_validation_loss\": 14.644786198774655,\n",
            "  \"test__ner_precision-Method\": 0.7967211031327445,\n",
            "  \"test__ner_recall-Method\": 0.7697218982135183,\n",
            "  \"test__ner_f1-measure-Method\": 0.7829888215824143,\n",
            "  \"test__ner_precision-Material\": 0.4916833000665336,\n",
            "  \"test__ner_recall-Material\": 0.45393120393120395,\n",
            "  \"test__ner_f1-measure-Material\": 0.4720536569785512,\n",
            "  \"test__ner_precision-Task\": 0.683387858960378,\n",
            "  \"test__ner_recall-Task\": 0.7017543859649122,\n",
            "  \"test__ner_f1-measure-Task\": 0.6924493554327308,\n",
            "  \"test__ner_precision-Metric\": 0.7985611510791367,\n",
            "  \"test__ner_recall-Metric\": 0.6774193548387096,\n",
            "  \"test__ner_f1-measure-Metric\": 0.7330188679244787,\n",
            "  \"test__ner_precision\": 0.7530379131561893,\n",
            "  \"test__ner_recall\": 0.727091399741875,\n",
            "  \"test__ner_f1-measure\": 0.7398372366038064,\n",
            "  \"test__loss_ner\": 13.950788497924805,\n",
            "  \"test_validation_metric\": 0.7398372366038064,\n",
            "  \"test_loss\": 13.950791804600293\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!bash scirex/commands/train_scirex_model.sh CUDA_DEVICE=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "owN9n8-1CmVs",
        "outputId": "04d72b0d-4586-4287-8f9c-dbf083fa90bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYL83L3MHDKY",
        "outputId": "9497b70d-061c-4673-e0a8-b2dca1a542ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TJoNq8t-aUC",
        "outputId": "7bb98e42-a497-4ec8-a282-402449c185e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 12 07:55:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnSAyN4S-g9x",
        "outputId": "a50507b0-a83e-431e-c7e0-594a417f6167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "82hvOeb4-vdz",
        "outputId": "487eed8d-2b31-48f0-dc89-fe2d5c8259d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQq0h5MrbsZo"
      },
      "outputs": [],
      "source": [
        "from allennlp.data.iterators.data_iterator import DataIterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqn2Vnvobwug",
        "outputId": "e006777e-7ee2-4be7-dd61-c1c0607e792b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SciREX/scirex\n"
          ]
        }
      ],
      "source": [
        "cd /content/SciREX/scirex"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dIX8Bxo07tqF",
        "outputId": "194b0a3a-855d-4bdf-cc16-16a2d12de982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/SciREX/scirex'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa5Mprjp7xRl",
        "outputId": "44509e4a-ac16-41b2-c13c-09b759912c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SciREX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIi9hAW2bzUo",
        "outputId": "1c384802-ca9c-471b-9b12-0d8236f78f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bash: train_scirex_model.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!bash train_scirex_model.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !zip -r outputs.zip outputs/ "
      ],
      "metadata": {
        "id": "B0jwOw95_kjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebf64ef-430a-4b74-9d14-34ee3f327ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: outputs/ (stored 0%)\n",
            "  adding: outputs/pwc_outputs/ (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/ (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/ (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/training_state_epoch_2.th (deflated 9%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/model_state_epoch_2.th (deflated 7%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/stdout.log (deflated 93%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/vocabulary/ (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/vocabulary/ner_type_labels.txt (deflated 53%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/vocabulary/section_feature_labels.txt (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/vocabulary/span_type_labels.txt (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/vocabulary/non_padded_namespaces.txt (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/metrics_epoch_1.json (deflated 78%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/config.json (deflated 71%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/metrics.json (deflated 77%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/model.tar.gz (deflated 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/log/ (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/log/train/ (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/log/train/events.out.tfevents.1676100337.2954eaa5c9e8 (deflated 85%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/log/validation/ (stored 0%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/log/validation/events.out.tfevents.1676100337.2954eaa5c9e8 (deflated 61%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/stderr.log (deflated 85%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/metrics_epoch_2.json (deflated 78%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/metrics_epoch_0.json (deflated 77%)\n",
            "  adding: outputs/pwc_outputs/experiment_scirex_full/CUDA_DEVICE=0/best.th (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TzIOnj68tqLo",
        "outputId": "bf534884-ae59-46e9-e5ee-15655677c382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/SciREX'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GTpCSunKvVML",
        "outputId": "18270473-2934-4f59-8de0-660fe2644962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/SciREX'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! scirex_archive=outputs/pwc_outputs/experiment_scirex_full/main \\\n",
        "scirex_coreference_archive=outputs/pwc_outputs/experiment_coreference/main \\\n",
        "cuda_device=0 \\\n",
        "bash scirex/commands/predict_scirex_model.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwKXBZzMsexU",
        "outputId": "8056abbd-ac19-48f5-a5fc-c2268da6940f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting NER\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dtype=np.int):\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, positive=False):\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
            "['scirex/predictors/predict_ner.py', 'outputs/pwc_outputs/experiment_scirex_full/main', 'scirex_dataset/release_data/test.jsonl', 'test_outputs//ner_predictions.jsonl', '0']\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  EPS = np.finfo(np.float).eps\n",
            "2023-02-11 09:55:57,272:INFO:Loading Model from outputs/pwc_outputs/experiment_scirex_full/main\n",
            "Traceback (most recent call last):\n",
            "  File \"scirex/predictors/predict_ner.py\", line 169, in <module>\n",
            "    main()\n",
            "  File \"scirex/predictors/predict_ner.py\", line 165, in main\n",
            "    predict(archive_folder, test_file, output_file, cuda_device)\n",
            "  File \"scirex/predictors/predict_ner.py\", line 73, in predict\n",
            "    archive = load_archive(archive_file, cuda_device)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/allennlp/models/archival.py\", line 170, in load_archive\n",
            "    resolved_archive_file = cached_path(archive_file)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/allennlp/common/file_utils.py\", line 106, in cached_path\n",
            "    raise FileNotFoundError(\"file {} not found\".format(url_or_filename))\n",
            "FileNotFoundError: file outputs/pwc_outputs/experiment_scirex_full/main/model.tar.gz not found\n",
            "Evaluating on all Predicted steps \n",
            "Traceback (most recent call last):\n",
            "  File \"scirex/evaluation_scripts/scirex_relation_evaluate.py\", line 7, in <module>\n",
            "    from scirex.metrics.clustering_metrics import match_predicted_clusters_to_gold\n",
            "ModuleNotFoundError: No module named 'scirex'\n",
            "Evaluating on all predicted steps with filtering using gold salient clusters\n",
            "Traceback (most recent call last):\n",
            "  File \"scirex/evaluation_scripts/scirex_relation_evaluate.py\", line 7, in <module>\n",
            "    from scirex.metrics.clustering_metrics import match_predicted_clusters_to_gold\n",
            "ModuleNotFoundError: No module named 'scirex'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bash scirex/commands/predict_scirex_model.sh"
      ],
      "metadata": {
        "id": "ofAMmlwCsg6D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}